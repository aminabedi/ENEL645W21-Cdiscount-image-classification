{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a90a8sE_abWS"
   },
   "source": [
    "# Project - Cdiscount Image Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQSDw9jO3VP9"
   },
   "source": [
    "# Data ingestion\n",
    "The primary training set is a 57GB bson file, having ~15 Million images (180x180 images in Base64 format) of ~7.06 Million products. We have imported the dataset into a MongoDB instance on a VPS, so we were able to query among the records.\n",
    "We have chosen 100 categories, which overally consist of ~246K images of ~110K products.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NunsFV4jxHc"
   },
   "source": [
    "## Dataset preparation\n",
    "\n",
    "First we need to ensure that the \"gdown\" library is installed and accessible in the environment and download the train_medium data from Google Drive,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVPqZO_a0mc5",
    "outputId": "9221f1a1-eacc-4b29-ff9d-bb2f0b435772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1F6Xf4yiYxeFEN6qhrL3YBNs0Vhx0bXJ1\n",
      "To: /content/train_shuffled_100cat.csv\n",
      "1.62GB [00:09, 178MB/s]\n"
     ]
    }
   ],
   "source": [
    "! pip install gdown && gdown --id 1F6Xf4yiYxeFEN6qhrL3YBNs0Vhx0bXJ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SpivBrTeja1s"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e6a2b35e01ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m \u001b[0;31m# tensorflow datasets - pip install tensorflow-datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds # tensorflow datasets - pip install tensorflow-datasets\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform # pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8ZkDULUNaOV"
   },
   "source": [
    "## Note for the team\n",
    "Since the original dataset is pretty large, I've created a subset file containing ~250K photos in 100 categories, but it is still so large that it may not fit into the memory, so I've used the below parameters to load a fitable subset accordingly, please read the comments of each variable careflully, and do not change the loading code please, just set the values of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xOFXWL41NaOW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import base64\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni7OSQvyNaOW"
   },
   "source": [
    "Run the cell below if you have a gpu that you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4IVK6e4JNaOW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices):\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9gg39dI6f-E"
   },
   "source": [
    "## 1. Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NM2fnrxPNaOX"
   },
   "outputs": [],
   "source": [
    "CONVERT_TO_NP_ARRAY= False   # Wether convert the Base64 string to (180,180,3) arrays or keep the Base64 string.\n",
    "\n",
    "REPLACE_BASE64_SPECIAL_CHARS = True # If you have the base64 decoding layer in your model, \n",
    "                                    # you need to set this to True to replace the two special characters in Base64 that is incompatible with tf image reader\n",
    "\n",
    "LOADING_MODE = \"all\" \n",
    "                             # num_records: Loads the first NUM_RECORDS in the dataset and calculates NUM_CATEGORIES dynamically\n",
    "                             # num_categories: Loads first NUM_CATEGORIES and calculates NUM_RECORDS dynamically\n",
    "                             # all: Loads all the 250K images, ignores all parameters below\n",
    "                            \n",
    "    \n",
    "NUM_RECORDS = 3000           # Only used when the mode is set to num_records\n",
    "NUM_CATEGORIES = 100           # Only used when the mode is set to num_category\n",
    "MAX_RECORDS_PER_CATEGORY = 700 # if not zero, will ensure that there is no more per category in the dataframe, won't work when mode is set to all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Em6RkMvGNaOX"
   },
   "outputs": [],
   "source": [
    "def get_array_from_base64(img, shape=(180,180,3)):\n",
    "    print(img)\n",
    "    return np.array(\n",
    "        Image.open(\n",
    "            io.BytesIO(\n",
    "                base64.b64decode(\n",
    "                    img.replace('_', '/').replace('-', '+') if REPLACE_BASE64_SPECIAL_CHARS else img\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ).reshape(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0t1RKH_QNaOX",
    "outputId": "13d7855e-f4e2-4526-c9af-ecb7ca25fd29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num records: 3000\n",
      "Num categories: 99\n",
      "Training df shape: (246261, 4)\n",
      "Mem used by images: 1613 MB\n"
     ]
    }
   ],
   "source": [
    "FILE_NAME= \"../train_shuffled_100cat.csv\"\n",
    "header = 3\n",
    "\n",
    "df = pd.read_csv(FILE_NAME, header=3, nrows=0)\n",
    "\n",
    "if LOADING_MODE == \"all\":\n",
    "    df = pd.read_csv(FILE_NAME, header=header)\n",
    "\n",
    "if LOADING_MODE == \"num_records\":\n",
    "    reader = pd.read_csv(FILE_NAME, header=header, chunksize=min(NUM_RECORDS, 100))\n",
    "    for chunk in reader:\n",
    "        df = df.append(chunk, ignore_index=True)\n",
    "        if MAX_RECORDS_PER_CATEGORY:\n",
    "            for cat in df[\"category_id\"].unique():\n",
    "                if len(df[df[\"category_id\"] == cat]) > MAX_RECORDS_PER_CATEGORY:\n",
    "                    removed_rows = df[df[\"category_id\"] == cat][MAX_RECORDS_PER_CATEGORY:]\n",
    "                    df = df.drop(removed_rows.index)\n",
    "        if df.shape[0]>=NUM_RECORDS:\n",
    "            df = df.head(NUM_RECORDS)\n",
    "            break\n",
    "    \n",
    "elif LOADING_MODE == \"num_categories\": \n",
    "    reader = pd.read_csv(FILE_NAME, header=header, chunksize=100)\n",
    "    for chunk in reader:\n",
    "        df = df.append(chunk, ignore_index=True)\n",
    "        if df[\"category_id\"].nunique() > NUM_CATEGORIES:\n",
    "            break\n",
    "    if MAX_RECORDS_PER_CATEGORY:\n",
    "        for cat in df[\"category_id\"].unique():\n",
    "            if len(df[df[\"category_id\"] == cat]) > MAX_RECORDS_PER_CATEGORY:\n",
    "                removed_rows = df[df[\"category_id\"] == cat][MAX_RECORDS_PER_CATEGORY:]\n",
    "                df = df.drop(removed_rows.index)\n",
    "\n",
    "    cat_removed = df[\"category_id\"].unique()[NUM_CATEGORIES:]\n",
    "    df = df.loc[~df['category_id'].isin(cat_removed)]\n",
    "    NUM_RECORDS= df.shape[0]\n",
    "\n",
    "if CONVERT_TO_NP_ARRAY:        \n",
    "    df[\"image\"] = df[\"image\"].apply(\n",
    "                    lambda x: get_array_from_base64(x)\n",
    "                )\n",
    "if REPLACE_BASE64_SPECIAL_CHARS:\n",
    "    df[\"image\"] = df[\"image\"].apply(\n",
    "                    lambda x: x.replace('/', '_').replace('+', '-')\n",
    "                )\n",
    "    \n",
    "NUM_CATEGORIES = df['category_id'].nunique()\n",
    "\n",
    "categories = df['category_id'].unique()\n",
    "categories.sort()\n",
    "category_id_map = {k: v for v, k in enumerate(categories)}\n",
    "df[\"class\"] = df[\"category_id\"].apply(lambda x: category_id_map[x])\n",
    "\n",
    "print(\"Num records:\", NUM_RECORDS)\n",
    "print(\"Num categories:\", NUM_CATEGORIES)\n",
    "print(\"Training df shape:\", df.shape)\n",
    "print(\"Mem used by images:\", int(sum(df[\"image\"].apply(lambda x: x.nbytes if type(x)!=str else len(x))/10 ** 6)), \"MB\")\n",
    "# print(len(df.at[0, \"image\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7qa5Rle5NaOY"
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1,random_state = 100)\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W94KecGabAoB",
    "outputId": "f43bd2a0-8bff-43fe-c0ec-9cc80fe39319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246261,) (246261,) [22 50 60 27 58 91  9 44 57 60]\n"
     ]
    }
   ],
   "source": [
    "X_dev = np.stack(df[\"image\"]) if CONVERT_TO_NP_ARRAY else np.array(df[\"image\"])\n",
    "Y_dev = np.array(df[\"class\"])\n",
    "print(X_dev.shape,Y_dev.shape, Y_dev[-10:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0KQWQTcbjUx"
   },
   "source": [
    "## 2. Explore your data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSVj9Xm2_Qsl"
   },
   "source": [
    "Showing 10 samples from dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xxMSDDkB6f-G",
    "outputId": "e995679c-7060-48d9-cc39-18870a9e88d9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 12000x6000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6), dpi=1000)\n",
    "indexes = np.arange(len(X_dev))\n",
    "np.random.shuffle(indexes)\n",
    "if CONVERT_TO_NP_ARRAY:\n",
    "    for idx in range(10):\n",
    "      plt.subplot(2, 5, idx + 1)\n",
    "      plt.imshow(X_dev[indexes[idx]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hExX4dqr_XNZ"
   },
   "source": [
    "#Splitting dev set into train/val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fv6OSGIO_bfG",
    "outputId": "6ab9a080-6f69-4529-885e-2941635a9671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (184695,)\n",
      "X_val: (49252,)\n",
      "X_test: (12314,)\n"
     ]
    }
   ],
   "source": [
    "num_train = int(len(X_dev) * .75) #= splitting point of train/val set\n",
    "num_val = int(len(X_dev) * .2)\n",
    "num_test = len(X_dev) - num_train - num_val\n",
    "\n",
    "X_train = X_dev[indexes[:num_train]]\n",
    "Y_train = Y_dev[indexes[:num_train]]\n",
    "\n",
    "X_val = X_dev[indexes[num_train:-num_test]]\n",
    "Y_val = Y_dev[indexes[num_train:-num_test]]\n",
    "\n",
    "X_test = X_dev[indexes[-num_test:]]\n",
    "Y_test = Y_dev[indexes[-num_test:]]\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sswDlu56f-H"
   },
   "source": [
    "## 3. Represent your labels using one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XbQ9b9TA6f-I"
   },
   "outputs": [],
   "source": [
    "# Y_train_oh = tf.keras.utils.to_categorical(Y_train, num_classes = NUM_CATEGORIES)\n",
    "# Y_val_oh = tf.keras.utils.to_categorical(Y_val, num_classes = NUM_CATEGORIES)\n",
    "# Y_test_oh = tf.keras.utils.to_categorical(Y_test, num_classes = NUM_CATEGORIES)\n",
    "\n",
    "\n",
    "# print(\"Y_train\",  Y_train[:3])\n",
    "# print(\"Y_train_oh:\",  Y_train_oh[:3])\n",
    "\n",
    "\n",
    "# print(\"TRAIN:\", X_train.shape, Y_train.shape, Y_train_oh.shape)\n",
    "# print(\"VAL:\", X_val.shape, Y_val.shape, Y_val_oh.shape)\n",
    "# print(\"TEST:\", X_test.shape, Y_test.shape, Y_test_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 184695 images belonging to 99 classes.\n",
      "Found 184695 images belonging to 99 classes.\n",
      "Found 49252 images belonging to 99 classes.\n",
      "Found 12314 images belonging to 99 classes.\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT=\"../data-100cat/\"\n",
    "seed = 909 # (IMPORTANT) to input image and corresponding target with same augmentation parameter.\n",
    "\n",
    "gen_params = {\"rescale\":1.0/255,\"featurewise_center\":False,\"samplewise_center\":False,\"featurewise_std_normalization\":False,\\\n",
    "              \"samplewise_std_normalization\":False,\"zca_whitening\":False,\"rotation_range\":20,\"width_shift_range\":0.1,\"height_shift_range\":0.1,\\\n",
    "              \"shear_range\":0.2, \"zoom_range\":0.1,\"horizontal_flip\":True,\"fill_mode\":'constant',\\\n",
    "               \"cval\": 0}\n",
    "\n",
    "train_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**gen_params) \n",
    "\n",
    "train_image_generator = train_image_datagen.flow_from_directory(DATA_ROOT+\"train/\",\n",
    "                                                    class_mode=\"categorical\",  classes=[str(i) for i in range(99)], target_size=(180, 180), batch_size = 16,seed=seed,shuffle = True)\n",
    "\n",
    "train_image_datagen_non_augmented = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) \n",
    "\n",
    "train_image_generator_non_augmented = train_image_datagen_non_augmented.flow_from_directory(DATA_ROOT+\"train/\",\n",
    "                                                    class_mode=\"categorical\",  classes=[str(i) for i in range(99)], target_size=(180, 180), batch_size = 16,seed=seed,shuffle = True)\n",
    "\n",
    "\n",
    "val_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) \n",
    "\n",
    "val_image_generator = val_image_datagen.flow_from_directory(DATA_ROOT+\"val/\",\n",
    "                                                     class_mode=\"categorical\",  classes=[str(i) for i in range(99)],batch_size = 16,seed=seed, target_size=(180, 180),color_mode='rgb',shuffle = True)\n",
    "\n",
    "test_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) \n",
    "\n",
    "test_image_generator = val_image_datagen.flow_from_directory(DATA_ROOT+\"test/\",\n",
    "                                                     class_mode=\"categorical\", classes=[str(i) for i in range(99)],batch_size = 16,seed=seed, target_size=(180, 180),color_mode='rgb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfQzijUy6f-J"
   },
   "source": [
    "## 4. Data scaling and Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_n4rt2mGNaOa"
   },
   "source": [
    "### Preprocess_and_decode:\n",
    "It is a function which applies to each image input to the model, it first decodes the JPEG base64 encoded image to a tensor, then it scales it based on the sample's min, max, mean and std.\n",
    "Since we are normalizing our data per sample, our normalization is row-wise and a column-wise normalization is not yet an option. We can do this by preprocessing our dataset by batching it if that seemed necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MCdL608yNaOb"
   },
   "outputs": [],
   "source": [
    "def preprocess_and_decode(img_str, new_shape=(180,180), scaling_mode = 2 ): #scaling_mode= 0: disabled, 1: min-max normalization, 2: standardization\n",
    "    img = tf.io.decode_base64(img_str)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, new_shape, method=tf.image.ResizeMethod.BILINEAR)\n",
    "   \n",
    "    if scaling_mode == 1: \n",
    "      img_min = tf.math.reduce_min(img, axis=None, keepdims=False, name=None)\n",
    "      img_max = tf.math.reduce_max(img, axis=None, keepdims=False, name=None)\n",
    "      img = ( img - img_min ) / (img_max - img_min)\n",
    "      \n",
    "    elif scaling_mode == 2:\n",
    "      img_mean = tf.math.reduce_mean(img, axis=None, keepdims=False, name=None)\n",
    "      img_std = tf.math.reduce_std(img, axis=None, keepdims=False, name=None)\n",
    "      img = ( img - img_mean ) / img_std\n",
    "      \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaUdBkorNaOb"
   },
   "source": [
    "### About data augmentation\n",
    "Since our dataset consist of 2-4 image per product, and we have over 2K images per category, data augmentation seems unnecessary.\n",
    "I still have not found a way to do data augmentation on Base64 strings(remember I moved to Base64-> tensor decoding inside the model itself to enable us to do batch training on the whole dataset), but if we come to a need for it, there is definitly a way to do that!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibIuCJatMuDE"
   },
   "source": [
    "## 4. Define your callbacks (save your model, patience, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DLM8gBM-p24a"
   },
   "outputs": [],
   "source": [
    "model_name = \"vgg_project_after_finetuning_withoutaug_all_amin.h5\"\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 20)\n",
    "\n",
    "monitor = tf.keras.callbacks.ModelCheckpoint(model_name, monitor='val_loss',\\\n",
    "                                             verbose=0,save_best_only=True,\\\n",
    "                                             save_weights_only=True,\\\n",
    "                                             mode='min')\n",
    "# Learning rate schedule\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch%4 == 0 and epoch!= 0:\n",
    "        lr = lr/2\n",
    "    return lr\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3E9YADeNtGF"
   },
   "source": [
    "## 6. Transfer Learning\n",
    "\n",
    "6.1 Choose and load your pretrained model without the top (i.e., the prediction part, usually the fully connected layers)\n",
    "\n",
    "6.2. Freeze the layers (i.e., make them non-trainable) of your pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ysy_-qIgNvoc",
    "outputId": "7c4f4347-dc04-4224-9523-9d78e64fc8a9"
   },
   "outputs": [],
   "source": [
    "# Dimensions we will resize the images\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "k = 99\n",
    "# input64 = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "# img_tensor = tf.keras.layers.Lambda(\n",
    "#     lambda img: tf.map_fn(lambda im: preprocess_and_decode(im[0]), img, dtype=\"float32\"))(input64)\n",
    "input_layer = tf.keras.layers.Input(shape=(img_width, img_height, 3), dtype=\"float32\")\n",
    "base_model = tf.keras.applications.VGG16(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(img_height, img_width, 3),\n",
    "    include_top=False) \n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H0RXNaJNx6d"
   },
   "source": [
    "6.3. Add a top (i.e., the prediction layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjBs5BDpN0gB",
    "outputId": "7d306b41-2f4a-4ba2-dc06-2e540fcbd134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 180, 180, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, 5, 5, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 99)                1267299   \n",
      "=================================================================\n",
      "Total params: 15,981,987\n",
      "Trainable params: 1,267,299\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# input_image = tf.keras.Input(shape=(img_height, img_width, 3))\n",
    "x1 = base_model(input_layer, training=False)\n",
    "x2 = tf.keras.layers.Flatten()(x1)\n",
    "out = tf.keras.layers.Dense(k,activation = 'softmax')(x2) # 99 categories\n",
    "# out = tf.keras.layers.Dense(len(class_names),activation = 'softmax')(x2)\n",
    "model = tf.keras.Model(inputs = input_layer, outputs =out)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TwIJHQdu3o0Z"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRP94UYAN4Bs",
    "outputId": "af915712-c2ac-48aa-d47b-7ff895d8b6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(train_image_generator_non_augmented,epochs = 20, \\\n",
    "          verbose = 1, callbacks= [early_stop, monitor, lr_schedule],validation_data=(val_image_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSaqHUanY-RB"
   },
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afeRwubQZAap",
    "outputId": "a086ce17-9bde-499c-e781-32fe99410cbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n",
      "385/385 [==============================] - 32s 82ms/step - loss: 0.2325 - accuracy: 0.9453\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(model_name)\n",
    "print(\"LOADED\")\n",
    "metrics = model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuNAM0SGY7wx"
   },
   "source": [
    "Fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGZNOieaN42k",
    "outputId": "3528aa42-ede4-4a91-80b3-41fec26e71a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 180, 180, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, 5, 5, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 99)                1267299   \n",
      "=================================================================\n",
      "Total params: 15,981,987\n",
      "Trainable params: 15,981,987\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.VGG16(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(img_height, img_width, 3),\n",
    "    include_top=False) \n",
    "base_model.trainable = True\n",
    "input64 = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "img_tensor = tf.keras.layers.Lambda(\n",
    "    lambda img: tf.map_fn(lambda im: preprocess_and_decode(im[0]), img, dtype=\"float32\"))(input64)\n",
    "# input_image = tf.keras.Input(shape=(img_height, img_width, 3))\n",
    "x1 = base_model(img_tensor, training=True)\n",
    "x2 = tf.keras.layers.Flatten()(x1)\n",
    "out = tf.keras.layers.Dense(k,activation = 'softmax')(x2)\n",
    "model = tf.keras.Model(input64, out)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cjAvAw57ONV9",
    "outputId": "b4718794-232a-4081-c816-50128d15ae98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "5772/5772 [==============================] - 1831s 317ms/step - loss: 2.9883 - accuracy: 0.2642 - val_loss: nan - val_accuracy: 0.1059\n",
      "Epoch 2/7\n",
      "5772/5772 [==============================] - 1802s 312ms/step - loss: 3.4283 - accuracy: 0.1356 - val_loss: nan - val_accuracy: 0.1372\n",
      "Epoch 3/7\n",
      "5772/5772 [==============================] - 1800s 312ms/step - loss: 3.4193 - accuracy: 0.1371 - val_loss: nan - val_accuracy: 0.1372\n",
      "Epoch 4/7\n",
      "5772/5772 [==============================] - 1796s 311ms/step - loss: 3.4226 - accuracy: 0.1377 - val_loss: nan - val_accuracy: 0.1372\n",
      "Epoch 5/7\n",
      "5772/5772 [==============================] - 1791s 310ms/step - loss: 3.4152 - accuracy: 0.1367 - val_loss: nan - val_accuracy: 0.1372\n",
      "Epoch 6/7\n",
      "5772/5772 [==============================] - 1790s 310ms/step - loss: 3.4197 - accuracy: 0.1391 - val_loss: nan - val_accuracy: 0.1372\n",
      "Epoch 7/7\n",
      "5772/5772 [==============================] - 1793s 311ms/step - loss: 3.4197 - accuracy: 0.1383 - val_loss: nan - val_accuracy: 0.1372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f36fdd65f50>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-6),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.load_weights(model_name)\n",
    "model.fit(X_train, Y_train,batch_size = 32, epochs = 7, \\\n",
    "          verbose = 1, callbacks= [early_stop, monitor, lr_schedule],validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1dlyCsu6f-U"
   },
   "source": [
    "## 8. Test your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VfhhkxiNOSNN",
    "outputId": "f6bf515a-16ce-4401-aa8e-7c3549b14e29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385/385 [==============================] - 33s 86ms/step - loss: 0.2245 - accuracy: 0.9481\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(model_name)\n",
    "metrics = model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "bgVgUP83OWk5",
    "outputId": "1b3e05cc-770b-4385-ef0f-7581fd4fb255"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-65d01cb52b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwrong_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label: %d, predicted: %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwrong_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwrong_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'min'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ypred = model.predict(X_test).argmax(axis = 1)\n",
    "# wrong_indexes = np.where(Ypred != Y_test)[0]\n",
    "\n",
    "# # Disaplying some samples from the development set\n",
    "# sample_indexes = np.random.choice(np.arange(wrong_indexes.shape[0], dtype = int),size = 30, replace = False)\n",
    "# plt.figure(figsize = (24,18))\n",
    "# for (ii,jj) in enumerate(sample_indexes):\n",
    "#     plt.subplot(5,6,ii+1)\n",
    "#     aux = X_test[wrong_indexes[jj]]\n",
    "#     aux = (aux - aux.min())/(aux.max() - aux.min())\n",
    "#     plt.imshow(aux, cmap = \"gray\")\n",
    "#     plt.title(\"Label: %d, predicted: %d\" %(Y_test[wrong_indexes[jj]],Ypred[wrong_indexes[jj]]))\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "LfQzijUy6f-J"
   ],
   "history_visible": true,
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
